# -*- coding: utf-8 -*-
"""AAS_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P24bAiOzkp-H59ZQZcp0Dx1WDoCkMNQV
"""

!pip install -q torch torch_geometric

import time
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.nn import SAGEConv, GATConv, GCNConv
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef)
from pathlib import Path

BASE_DIR = Path('/content/drive/MyDrive/ColabNotebooks/AnomalyDetection')
SPLIT_GRAPHS_BASE_DIR = BASE_DIR / 'graphs'
ABLATION_SAVE_DIR = BASE_DIR / 'ss_dgnn_ablation_study_models'
ABLATION_SAVE_DIR.mkdir(parents=True, exist_ok=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SS_DGNN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dims, latent_dim, gnn_layer_type, activation_function):
        super(SS_DGNN, self).__init__()
        self.latent_dim = latent_dim
        self.num_layers = len(hidden_dims) + 1
        self.gnn_layer = self.get_gnn_layer(gnn_layer_type)
        self.activation = self.get_activation_function(activation_function)
        self.encoder_convs = nn.ModuleList()

        if self.num_layers == 1:
            self.encoder_convs.append(self.gnn_layer(input_dim, latent_dim))
        else:
            self.encoder_convs.append(self.gnn_layer(input_dim, hidden_dims[0]))
            for i in range(len(hidden_dims) - 1):
                self.encoder_convs.append(self.gnn_layer(hidden_dims[i], hidden_dims[i+1]))
            self.encoder_convs.append(self.gnn_layer(hidden_dims[-1], latent_dim))

        self.decoder_linear = nn.Linear(latent_dim, input_dim)

    def get_gnn_layer(self, layer_type):
        if layer_type == 'SAGEConv': return SAGEConv
        elif layer_type == 'GATConv': return GATConv
        elif layer_type == 'GCNConv': return GCNConv
        else: raise ValueError(f"Unknown layer: {layer_type}")

    def get_activation_function(self, activation_type):
        if activation_type == 'relu': return F.relu
        elif activation_type == 'leaky_relu': return F.leaky_relu
        elif activation_type == 'tanh': return torch.tanh
        elif activation_type == 'sigmoid': return torch.sigmoid
        else: raise ValueError(f"Unknown activation: {activation_type}")

    def encode(self, x, edge_index):
        for i, conv_layer in enumerate(self.encoder_convs):
            x = conv_layer(x, edge_index)
            if i < self.num_layers - 1:
                x = self.activation(x)
        return x

    def decode(self, z):
        return self.decoder_linear(z)

    def forward(self, data):
        z = self.encode(data.x, data.edge_index)
        return self.decode(z)

def get_reconstruction_errors(graph_list, model, device):
    errors = []
    model.eval()
    with torch.no_grad():
        for graph in graph_list:
            if graph.x.numel() == 0:
                errors.append(0.0)
                continue
            data = graph.to(device)
            reconstructed_x = model(data)
            error = F.mse_loss(reconstructed_x, data.x, reduction='mean').item()
            errors.append(error)
    return np.array(errors)

print("Loading graphs for Ablation Study (Activation: Sigmoid)...")
try:
    train_benign_graphs = torch.load(SPLIT_GRAPHS_BASE_DIR / 'train_graphs_benign.pt', weights_only=False)
    val_benign_graphs = torch.load(SPLIT_GRAPHS_BASE_DIR / 'val_graphs_benign.pt', weights_only=False)
    test_graphs = torch.load(SPLIT_GRAPHS_BASE_DIR / 'test_graphs_mixed.pt', weights_only=False)

    train_benign_graphs = [g.to(device) for g in train_benign_graphs]
    val_benign_graphs = [g.to(device) for g in val_benign_graphs]
    test_graphs = [g.to(device) for g in test_graphs]

    input_dim = train_benign_graphs[0].x.shape[1]
    print(f"Loaded successfully. Input Dim: {input_dim}")

except FileNotFoundError:
    print(f"Error: Graph files not found at {SPLIT_GRAPHS_BASE_DIR}")
    exit()

ABLATION_LAYER_TYPE = 'SAGEConv'
HIDDEN_DIMS = []
LATENT_DIM = 128
ACTIVATION = 'sigmoid'
BATCH_SIZE = 16
LR = 0.004937790467999321
OPTIMIZER_NAME = 'Adam'
EPOCHS = 100
PATIENCE = 29

FINAL_MODEL_PATH = ABLATION_SAVE_DIR / 'ss_dgnn_sigmoid_ablation.pt'

model = SS_DGNN(input_dim, HIDDEN_DIMS, LATENT_DIM, ABLATION_LAYER_TYPE, ACTIVATION).to(device)

if OPTIMIZER_NAME == 'Adam':
    optimizer = optim.Adam(model.parameters(), lr=LR)
elif OPTIMIZER_NAME == 'RMSprop':
    optimizer = optim.RMSprop(model.parameters(), lr=LR)
elif OPTIMIZER_NAME == 'SDG':
    optimizer = optim.SGD(model.parameters(), lr=LR)
else:
    raise ValueError(f"Unknown optimizer: {OPTIMIZER_NAME}")

criterion = nn.MSELoss()
train_loader = DataLoader(train_benign_graphs + val_benign_graphs, batch_size=BATCH_SIZE, shuffle=True)

best_val_loss = float('inf')
patience_counter = 0
start_time = time.time()
TEMP_MODEL_FILE = 'ablation_temp_best_sigmoid.pt'

for epoch in range(1, EPOCHS + 1):
    model.train()
    total_loss = 0
    for data in train_loader:
        if data.x.numel() == 0: continue
        data = data.to(device)
        optimizer.zero_grad()
        recon = model(data)
        loss = criterion(recon, data.x)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * data.num_graphs

    avg_train_loss = total_loss / len(train_loader.dataset)
    val_errors = get_reconstruction_errors(val_benign_graphs, model, device)
    avg_val_loss = val_errors.mean()

    if epoch % 10 == 0:
        print(f"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")

    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), TEMP_MODEL_FILE)
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print(f"Early stopping at epoch {epoch}")
            break

training_time = time.time() - start_time
print(f"Training finished in {training_time:.2f} seconds.")

print(f"Saving best Sigmoid model to: {FINAL_MODEL_PATH}")
model.load_state_dict(torch.load(TEMP_MODEL_FILE, weights_only=True))
torch.save(model.state_dict(), FINAL_MODEL_PATH)

model.eval()
val_errors = get_reconstruction_errors(val_benign_graphs, model, device)
threshold = np.percentile(val_errors, 95.0)

inference_start = time.time()
test_errors = get_reconstruction_errors(test_graphs, model, device)
inference_time = time.time() - inference_start

test_labels = np.array([g.y.item() for g in test_graphs])
preds = (test_errors > threshold).astype(int)

acc = accuracy_score(test_labels, preds)
precision = precision_score(test_labels, preds)
recall = recall_score(test_labels, preds)
f1 = f1_score(test_labels, preds)
mcc = matthews_corrcoef(test_labels, preds)
auc = roc_auc_score(test_labels, test_errors)

print(f"=== RESULTS FOR ACTIVATION = Sigmoid ===")
print(f"Accuracy:       {acc:.4f}")
print(f"Precision:      {precision:.4f}")
print(f"Recall:         {recall:.4f}")
print(f"F1-Score:       {f1:.4f}")
print(f"MCC:            {mcc:.4f}")
print(f"ROC AUC:        {auc:.4f}")
print(f"Training Time:  {training_time:.4f} s")
print(f"Inference Time: {inference_time:.4f} s")
print("==========================================")
print(f"Model saved successfully at: {FINAL_MODEL_PATH}")