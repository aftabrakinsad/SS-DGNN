# -*- coding: utf-8 -*-
"""MLP_Autoencoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lICib0KPb1_txRFi0lcyjBTEtQq0hnxu
"""

!pip install -q torch torch_geometric

import time
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, matthews_corrcoef
)
from pathlib import Path

BASE_DIR = Path('/content/drive/MyDrive/ColabNotebooks/AnomalyDetection')
SPLIT_GRAPHS_BASE_DIR = BASE_DIR / 'graphs'
BASELINE_SAVE_DIR = BASE_DIR / 'baseline_models'
BASELINE_SAVE_DIR.mkdir(parents=True, exist_ok=True)

FINAL_MODEL_PATH = BASELINE_SAVE_DIR / 'mlp_autoencoder.pt'

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class MLP_AE(nn.Module):
    def __init__(self, input_dim, latent_dim=128):
        super(MLP_AE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, latent_dim),
            nn.Tanh()
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)

def flatten_graphs(graph_list):
    flat_data = []
    labels = []
    for g in graph_list:
        if g.x.numel() == 0: continue
        pooled_features = g.x.mean(dim=0).cpu().numpy()
        flat_data.append(pooled_features)
        labels.append(g.y.item())
    return np.array(flat_data), np.array(labels)

print("Loading Graph Data for MLP Baseline...")
try:
    train_graphs = torch.load(SPLIT_GRAPHS_BASE_DIR / 'train_graphs_benign.pt', weights_only=False)
    val_graphs = torch.load(SPLIT_GRAPHS_BASE_DIR / 'val_graphs_benign.pt', weights_only=False)
    test_graphs = torch.load(SPLIT_GRAPHS_BASE_DIR / 'test_graphs_mixed.pt', weights_only=False)

    print("Flattening graphs into vectors...")
    X_train_raw, y_train = flatten_graphs(train_graphs + val_graphs)
    X_test_raw, y_test = flatten_graphs(test_graphs)

    print(f"Train Data Shape: {X_train_raw.shape}")
    print(f"Test Data Shape:  {X_test_raw.shape}")

    train_tensor = torch.tensor(X_train_raw, dtype=torch.float32).to(device)
    test_tensor = torch.tensor(X_test_raw, dtype=torch.float32).to(device)

except FileNotFoundError:
    print(f"Error: Graph files not found at {SPLIT_GRAPHS_BASE_DIR}")
    exit()

INPUT_DIM = X_train_raw.shape[1]
BATCH_SIZE = 16
LR = 0.004937790467999321
EPOCHS = 100

model = MLP_AE(INPUT_DIM, latent_dim=128).to(device)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.MSELoss()

train_loader = TorchDataLoader(TensorDataset(train_tensor), batch_size=BATCH_SIZE, shuffle=True)

print("--- Training MLP Autoencoder ---")
start_train = time.time()
model.train()

for epoch in range(1, EPOCHS + 1):
    total_loss = 0
    for batch in train_loader:
        x_batch = batch[0]
        optimizer.zero_grad()
        recon = model(x_batch)
        loss = criterion(recon, x_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x_batch.size(0)

    avg_loss = total_loss / len(train_loader.dataset)
    if epoch % 10 == 0:
        print(f"Epoch {epoch:03d} | Train Loss: {avg_loss:.6f}")

train_time = time.time() - start_train
print(f"Training finished in {train_time:.2f} seconds.")

torch.save(model.state_dict(), FINAL_MODEL_PATH)
print(f"Model saved to: {FINAL_MODEL_PATH}")

model.eval()
start_inf = time.time()

with torch.no_grad():
    recon_train = model(train_tensor)
    train_errors = torch.mean((recon_train - train_tensor) ** 2, dim=1).cpu().numpy()

    threshold = np.percentile(train_errors, 95.0)

    recon_test = model(test_tensor)
    test_errors = torch.mean((recon_test - test_tensor) ** 2, dim=1).cpu().numpy()

inf_time = time.time() - start_inf

preds = (test_errors > threshold).astype(int)

acc = accuracy_score(y_test, preds)
prec = precision_score(y_test, preds)
rec = recall_score(y_test, preds)
f1 = f1_score(y_test, preds)
mcc = matthews_corrcoef(y_test, preds)
auc = roc_auc_score(y_test, test_errors)

print(f"=== RESULTS FOR MLP AUTOENCODER =============")
print(f"Accuracy:       {acc:.4f}")
print(f"Precision:      {prec:.4f}")
print(f"Recall:         {rec:.4f}")
print(f"F1-Score:       {f1:.4f}")
print(f"MCC:            {mcc:.4f}")
print(f"ROC AUC:        {auc:.4f}")
print(f"Training Time:  {train_time:.4f} s")
print(f"Inference Time: {inf_time:.4f} s")
print("==============================================")