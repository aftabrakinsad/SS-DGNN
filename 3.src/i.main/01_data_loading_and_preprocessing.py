# -*- coding: utf-8 -*-
"""01_data_loading_and_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FJgWPdf_HaesZHF0bPL4k8Ls8nJKQ-Fr
"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

base_path = Path('/content/drive/MyDrive/ColabNotebooks/AnomalyDetection/data/rawData')
zip_path = base_path / 'NF-BoT-IoT-v2.zip'
extract_path = Path('/content/NF-BoT-IoT-v2')
csv_file_path = extract_path / 'data/NF-BoT-IoT-v2.2.csv'

if not extract_path.exists():
    print(f"Extracting {zip_path}...")
    !unzip -q {zip_path} -d /content/
else:
    print(f"Dataset already extracted at {extract_path}")

df = pd.read_csv(csv_file_path)

print(f"DataFrame shape: {df.shape}")

df.columns = df.columns.str.strip()
print("Column names after stripping whitespace:")
print(df.columns.tolist())

print("First 5 rows of the DataFrame:")
print(df.head())

print("DataFrame Info:")
df.info()

print("Value counts for the 'Attack' column:")
print(df['Attack'].value_counts())

df.replace([np.inf, -np.inf], np.nan, inplace=True)
print("Replaced infinite values with NaN.")

fillna_values = {}
numerical_cols = df.select_dtypes(include=np.number).columns
for col in numerical_cols:
    if df[col].isnull().any():
        fillna_values[col] = df[col].median()

if fillna_values:
    df.fillna(fillna_values, inplace=True)
    print("Filled NaN values in numerical columns using median.")
else:
    print("No NaN values found in numerical columns or all handled.")

print(f"Total NaNs after filling: {df.isnull().sum().sum()}")

print("Checking for remaining NaN values across the DataFrame:")
print(df.isnull().sum().sum())

print("DataFrame Info after cleaning:")
df.info()

numerical_features_to_scale = [
    'L4_SRC_PORT', 'L4_DST_PORT', 'PROTOCOL', 'L7_PROTO', 'IN_BYTES', 'IN_PKTS',
    'OUT_BYTES', 'OUT_PKTS', 'TCP_FLAGS', 'CLIENT_TCP_FLAGS', 'SERVER_TCP_FLAGS',
    'FLOW_DURATION_MILLISECONDS', 'DURATION_IN', 'DURATION_OUT', 'MIN_TTL',
    'MAX_TTL', 'LONGEST_FLOW_PKT', 'SHORTEST_FLOW_PKT', 'MIN_IP_PKT_LEN',
    'MAX_IP_PKT_LEN', 'SRC_TO_DST_SECOND_BYTES', 'DST_TO_SRC_SECOND_BYTES',
    'RETRANSMITTED_IN_BYTES', 'RETRANSMITTED_IN_PKTS', 'RETRANSMITTED_OUT_BYTES',
    'RETRANSMITTED_OUT_PKTS', 'SRC_TO_DST_AVG_THROUGHPUT', 'DST_TO_SRC_AVG_THROUGHPUT',
    'NUM_PKTS_UP_TO_128_BYTES', 'NUM_PKTS_128_TO_256_BYTES', 'NUM_PKTS_256_TO_512_BYTES',
    'NUM_PKTS_512_TO_1024_BYTES', 'NUM_PKTS_1024_TO_1514_BYTES', 'TCP_WIN_MAX_IN',
    'TCP_WIN_MAX_OUT', 'ICMP_TYPE', 'ICMP_IPV4_TYPE', 'DNS_QUERY_ID',
    'DNS_QUERY_TYPE', 'DNS_TTL_ANSWER', 'FTP_COMMAND_RET_CODE'
]

missing_features = set(numerical_features_to_scale) - set(df.columns)
if missing_features:
    print("Warning: These features are missing from the dataset:")
    print(missing_features)

numerical_features_to_scale = [col for col in numerical_features_to_scale if col in df.columns]

for col in numerical_features_to_scale:
    if df[col].dtype in ['int64', 'float64']:
        df[col] = df[col].astype('float32')

print(f"Number of numerical features to scale: {len(numerical_features_to_scale)}")
print(f"Numerical features: {numerical_features_to_scale}")

BENIGN_LABEL = 'Benign'
benign_data = df[df['Attack'] == BENIGN_LABEL]

scaler = MinMaxScaler()
scaler.fit(benign_data[numerical_features_to_scale])
df[numerical_features_to_scale] = scaler.transform(df[numerical_features_to_scale])

print("DataFrame head after feature scaling:")
print(df[numerical_features_to_scale].head())

label_encoder = LabelEncoder()
df['Attack_Type'] = label_encoder.fit_transform(df['Attack'])

print("Mapping of 'Attack' types to numerical IDs:")
for i, label in enumerate(label_encoder.classes_):
    print(f"'{label}' -> {i}")

print("Value counts for the new 'Attack_Type' column:")
print(df['Attack_Type'].value_counts())

df.drop(columns=['Attack'], inplace=True)

df.info()

output_dir = Path('/content/drive/MyDrive/ColabNotebooks/AnomalyDetection/data/processedData')
output_dir.mkdir(parents=True, exist_ok=True)
processed_csv_path = output_dir / 'processed_NF-BoT-IoT-v2.2.csv'
df.to_csv(processed_csv_path, index=False)

print(f"Processed data saved to {processed_csv_path}")